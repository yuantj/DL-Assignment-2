{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebff7d12",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce215f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import tarfile\n",
    "import requests\n",
    "\n",
    "torch_dev = torch.accelerator.current_accelerator() if torch.accelerator.is_available() \\\n",
    "    else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341eac3",
   "metadata": {},
   "source": [
    "## Obtain the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bdc8da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target  source\n",
      "0          0      21\n",
      "1          0     905\n",
      "2          0     906\n",
      "3          0    1909\n",
      "4          0    1940\n",
      "...      ...     ...\n",
      "5424    1873     328\n",
      "5425    1873    1876\n",
      "5426    1874    2586\n",
      "5427    1876    1874\n",
      "5428    1897    2707\n",
      "\n",
      "[5429 rows x 2 columns]\n",
      "      paper_id  term_0  term_1  term_2  term_3  term_4  term_5  term_6  \\\n",
      "0          462       0       0       0       0       0       0       0   \n",
      "1         1911       0       0       0       0       0       0       0   \n",
      "2         2002       0       0       0       0       0       0       0   \n",
      "3          248       0       0       0       0       0       0       0   \n",
      "4          519       0       0       0       0       0       0       0   \n",
      "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "2703      2370       0       0       0       0       0       0       0   \n",
      "2704      2371       0       0       0       0       0       0       0   \n",
      "2705      2372       0       0       0       0       0       0       0   \n",
      "2706       955       0       0       0       0       1       0       0   \n",
      "2707       376       0       0       0       0       0       0       0   \n",
      "\n",
      "      term_7  term_8  ...  term_1424  term_1425  term_1426  term_1427  \\\n",
      "0          0       0  ...          0          0          1          0   \n",
      "1          0       0  ...          0          1          0          0   \n",
      "2          0       0  ...          0          0          0          0   \n",
      "3          0       0  ...          0          0          0          0   \n",
      "4          0       0  ...          0          0          0          0   \n",
      "...      ...     ...  ...        ...        ...        ...        ...   \n",
      "2703       0       0  ...          0          0          0          0   \n",
      "2704       0       0  ...          0          0          0          0   \n",
      "2705       0       0  ...          0          0          0          0   \n",
      "2706       0       0  ...          0          0          0          0   \n",
      "2707       0       0  ...          0          0          0          0   \n",
      "\n",
      "      term_1428  term_1429  term_1430  term_1431  term_1432  subject  \n",
      "0             0          0          0          0          0        2  \n",
      "1             0          0          0          0          0        5  \n",
      "2             0          0          0          0          0        4  \n",
      "3             0          0          0          0          0        4  \n",
      "4             0          0          0          0          0        3  \n",
      "...         ...        ...        ...        ...        ...      ...  \n",
      "2703          0          0          0          0          0        1  \n",
      "2704          0          0          0          0          0        1  \n",
      "2705          0          0          0          0          0        1  \n",
      "2706          0          0          0          0          0        0  \n",
      "2707          0          0          0          0          0        2  \n",
      "\n",
      "[2708 rows x 1435 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/7fb5s97s7g33v041x087dh3m0000gn/T/ipykernel_53327/2186911513.py:14: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=data_dir)\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"tutorial-dataset\"\n",
    "\n",
    "os.makedirs(path_prefix, exist_ok=True)\n",
    "\n",
    "response = requests.get(\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\", stream=True)\n",
    "with open(f\"{path_prefix}/cora.tgz\", \"wb\") as f:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        f.write(chunk)\n",
    "\n",
    "data_dir = f\"{path_prefix}/cora_extracted\"\n",
    "\n",
    "with tarfile.open(f\"{path_prefix}/cora.tgz\", \"r:gz\") as tar:\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    tar.extractall(path=data_dir)\n",
    "\n",
    "data_dir = os.path.join(data_dir, \"cora\")\n",
    "\n",
    "citations = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.cites\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"],\n",
    ")\n",
    "\n",
    "papers = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.content\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"],\n",
    ")\n",
    "\n",
    "class_values = sorted(papers[\"subject\"].unique())\n",
    "class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "\n",
    "print(citations)\n",
    "\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47569286",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a73104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain random indices\n",
    "random_indices = np.random.permutation(range(papers.shape[0]))\n",
    "\n",
    "# 50/50 split\n",
    "train_data = papers.iloc[random_indices[: len(random_indices) // 2]]\n",
    "test_data = papers.iloc[random_indices[len(random_indices) // 2 :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcaa6ee",
   "metadata": {},
   "source": [
    "### Prepare the graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345c767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape:          torch.Size([5429, 2])\n",
      "Node features shape:  torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "# Obtain paper indices which will be used to gather node states\n",
    "# from the graph later on when training the model\n",
    "train_indices = train_data[\"paper_id\"].to_numpy()\n",
    "test_indices = test_data[\"paper_id\"].to_numpy()\n",
    "\n",
    "# Obtain ground truth labels corresponding to each paper_id\n",
    "train_labels = train_data[\"subject\"].to_numpy()\n",
    "test_labels = test_data[\"subject\"].to_numpy()\n",
    "\n",
    "# Define graph, namely an edge tensor and a node feature tensor\n",
    "edges = torch.tensor(citations[[\"target\", \"source\"]].values, dtype=torch.long)\n",
    "node_states = torch.tensor(papers.sort_values(\"paper_id\").iloc[:, 1:-1].values, dtype=torch.float32)\n",
    "\n",
    "# Print shapes of the graph\n",
    "print(\"Edges shape:         \", edges.shape)\n",
    "print(\"Node features shape: \", node_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b663980",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d318dc7",
   "metadata": {},
   "source": [
    "### (Multi-head) graph attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2daa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(nn.Module):\n",
    "    def __init__(self, units, kernel_initializer=\"glorot_uniform\"):\n",
    "        super(GraphAttention, self).__init__()\n",
    "        self.units = units\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        # We delay weight creation until we see the input dimensions.\n",
    "        self.kernel = None\n",
    "        self.kernel_attention = None\n",
    "        self.built = False\n",
    "\n",
    "    def build(self, input_dim, device):\n",
    "        self.kernel = nn.Parameter(torch.empty(input_dim, self.units, device=device))\n",
    "        self.kernel_attention = nn.Parameter(torch.empty(self.units * 2, 1, device=device))\n",
    "\n",
    "        match self.kernel_attention:\n",
    "            case \"glroot_uniform\":\n",
    "                nn.init.xavier_uniform_(self.kernel)\n",
    "                nn.init.xavier_uniform_(self.kernel_attention)\n",
    "            case _:\n",
    "                # Fallback initializer; you might change this if needed.\n",
    "                nn.init.kaiming_uniform_(self.kernel, nonlinearity='leaky_relu')\n",
    "                nn.init.kaiming_uniform_(self.kernel_attention, nonlinearity='leaky_relu')\n",
    "        self.built = True\n",
    "\n",
    "    def forward(self, node_states, edges):\n",
    "        # Build weights on first forward pass (if not built)\n",
    "        if not self.built:\n",
    "            self.build(node_states.size(1), node_states.device)\n",
    "        \n",
    "        # (1) Linearly transform node states.\n",
    "        # Shape: (N, units)\n",
    "        node_states_transformed = torch.matmul(node_states, self.kernel)\n",
    "\n",
    "        # (2) Compute pairwise attention scores.\n",
    "        node_states_expanded = node_states_transformed[edges]\n",
    "        node_states_expanded = node_states_expanded.reshape(edges.size(0), -1)\n",
    "        attention_scores = F.leaky_relu(torch.matmul(node_states_expanded, self.kernel_attention))\n",
    "        attention_scores = attention_scores.squeeze(-1)\n",
    "\n",
    "        # (3) Normalize attention scores.\n",
    "        attention_scores_clipped = torch.clamp(attention_scores, -2, 2)\n",
    "        attention_scores_exp = torch.exp(attention_scores_clipped)\n",
    "        num_nodes = node_states.size(0)\n",
    "        attention_sum = torch.zeros(num_nodes, device=attention_scores.device)\n",
    "        attention_sum = attention_sum.index_add(0, edges[:, 0], attention_scores_exp)\n",
    "        normalized_attention = attention_scores_exp / attention_sum[edges[:, 0]]\n",
    "\n",
    "        # (4) Aggregate neighbor features: weighted sum over neighbors.\n",
    "        node_states_neighbors = node_states_transformed[edges[:, 1]]\n",
    "        weighted_neighbors = node_states_neighbors * normalized_attention.unsqueeze(1)\n",
    "        out = torch.zeros_like(node_states_transformed)\n",
    "        out = out.index_add(0, edges[:, 0], weighted_neighbors)\n",
    "        return out\n",
    "\n",
    "class MultiHeadGraphAttention(nn.Module):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\"):\n",
    "        super(MultiHeadGraphAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        # Create a ModuleList of independent attention heads.\n",
    "        self.attention_layers = nn.ModuleList(\n",
    "            [GraphAttention(units) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, atom_features, pair_indices):\n",
    "        # Compute the output for each attention head.\n",
    "        head_outputs = [att(atom_features, pair_indices) for att in self.attention_layers]\n",
    "\n",
    "        # Merge the outputs: either concatenate along the feature dimension or average them.\n",
    "        if self.merge_type == \"concat\":\n",
    "            # Concatenate along the last dimension.\n",
    "            out = torch.cat(head_outputs, dim=1)\n",
    "        else:\n",
    "            # Stack and then average over the head dimension.\n",
    "            out = torch.mean(torch.stack(head_outputs, dim=0), dim=0)\n",
    "        # Apply a ReLU nonlinearity (same as tf.nn.relu).\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c269b",
   "metadata": {},
   "source": [
    "### Implement training logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a107243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(nn.Module):\n",
    "    def __init__(self, node_states, edges, hidden_units, num_heads, num_layers, output_dim):\n",
    "        super(GraphAttentionNetwork, self).__init__()\n",
    "        self.node_states = node_states  # fixed graph node features\n",
    "        self.edges = edges              # fixed graph connectivity\n",
    "\n",
    "        # Preprocessing: linearly transform the initial node features.\n",
    "        in_features = node_states.size(1)\n",
    "        self.preprocess = nn.Linear(in_features, hidden_units * num_heads)\n",
    "\n",
    "        # Create a list of multi-head attention layers.\n",
    "        # Each layer applies attention and then a residual connection.\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads, merge_type=\"concat\")\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final output layer maps the aggregated representations to logits.\n",
    "        self.output_layer = nn.Linear(hidden_units * num_heads, output_dim)\n",
    "\n",
    "    def forward(self):\n",
    "        # Preprocess the node features.\n",
    "        x = self.preprocess(self.node_states)\n",
    "        x = F.relu(x)\n",
    "        # Sequentially pass through all attention layers with residual connections.\n",
    "        for att_layer in self.attention_layers:\n",
    "            x = att_layer(x, self.edges) + x\n",
    "        outputs = self.output_layer(x)\n",
    "        # The network produces raw logits.\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba632420",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a35fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/100 – Average Loss: 5.3878\n",
      "Epoch 2/100 – Average Loss: 4.7530\n",
      "Epoch 3/100 – Average Loss: 5.7000\n",
      "Epoch 4/100 – Average Loss: 2.6382\n",
      "Epoch 5/100 – Average Loss: 2.2462\n",
      "Epoch 6/100 – Average Loss: 2.6429\n",
      "Epoch 7/100 – Average Loss: 2.7237\n",
      "Epoch 8/100 – Average Loss: 2.4114\n",
      "Epoch 9/100 – Average Loss: 2.6545\n",
      "Epoch 10/100 – Average Loss: 2.3257\n",
      "Epoch 11/100 – Average Loss: 2.1453\n",
      "Epoch 12/100 – Average Loss: 2.1351\n",
      "Epoch 13/100 – Average Loss: 1.8951\n",
      "Epoch 14/100 – Average Loss: 1.8086\n",
      "Epoch 15/100 – Average Loss: 1.7305\n",
      "Epoch 16/100 – Average Loss: 1.7060\n",
      "Epoch 17/100 – Average Loss: 1.6611\n",
      "Epoch 18/100 – Average Loss: 1.6326\n",
      "Epoch 19/100 – Average Loss: 1.6203\n",
      "Epoch 20/100 – Average Loss: 1.5668\n",
      "Epoch 21/100 – Average Loss: 1.5204\n",
      "Epoch 22/100 – Average Loss: 1.5219\n",
      "Epoch 23/100 – Average Loss: 1.4933\n",
      "Epoch 24/100 – Average Loss: 1.5464\n",
      "Epoch 25/100 – Average Loss: 1.6475\n",
      "Epoch 26/100 – Average Loss: 1.9031\n",
      "Epoch 27/100 – Average Loss: 2.0474\n",
      "Epoch 28/100 – Average Loss: 1.9208\n",
      "Epoch 29/100 – Average Loss: 1.9201\n",
      "Epoch 30/100 – Average Loss: 1.6177\n",
      "Epoch 31/100 – Average Loss: 1.4996\n",
      "Epoch 32/100 – Average Loss: 1.4904\n",
      "Epoch 33/100 – Average Loss: 1.5356\n",
      "Epoch 34/100 – Average Loss: 1.4458\n",
      "Epoch 35/100 – Average Loss: 1.4588\n",
      "Epoch 36/100 – Average Loss: 1.4132\n",
      "Epoch 37/100 – Average Loss: 1.3945\n",
      "Epoch 38/100 – Average Loss: 1.4088\n",
      "Epoch 39/100 – Average Loss: 1.3623\n",
      "Epoch 40/100 – Average Loss: 1.3373\n",
      "Epoch 41/100 – Average Loss: 1.3293\n",
      "Epoch 42/100 – Average Loss: 1.3159\n",
      "Epoch 43/100 – Average Loss: 1.3042\n",
      "Epoch 44/100 – Average Loss: 1.2553\n",
      "Epoch 45/100 – Average Loss: 1.2179\n",
      "Epoch 46/100 – Average Loss: 1.2324\n",
      "Epoch 47/100 – Average Loss: 1.2784\n",
      "Epoch 48/100 – Average Loss: 1.2251\n",
      "Epoch 49/100 – Average Loss: 1.2240\n",
      "Epoch 50/100 – Average Loss: 1.2290\n",
      "Epoch 51/100 – Average Loss: 1.2122\n",
      "Epoch 52/100 – Average Loss: 1.3081\n",
      "Epoch 53/100 – Average Loss: 1.1974\n",
      "Epoch 54/100 – Average Loss: 1.3043\n",
      "Epoch 55/100 – Average Loss: 1.2441\n",
      "Epoch 56/100 – Average Loss: 1.1776\n",
      "Epoch 57/100 – Average Loss: 1.1807\n",
      "Epoch 58/100 – Average Loss: 1.1788\n",
      "Epoch 59/100 – Average Loss: 1.1425\n",
      "Epoch 60/100 – Average Loss: 1.0409\n",
      "Epoch 61/100 – Average Loss: 0.9878\n",
      "Epoch 62/100 – Average Loss: 0.9329\n",
      "Epoch 63/100 – Average Loss: 0.9271\n",
      "Epoch 64/100 – Average Loss: 0.8801\n",
      "Epoch 65/100 – Average Loss: 0.8914\n",
      "Epoch 66/100 – Average Loss: 0.8916\n",
      "Epoch 67/100 – Average Loss: 0.8719\n",
      "Epoch 68/100 – Average Loss: 0.8862\n",
      "Epoch 69/100 – Average Loss: 0.8800\n",
      "Epoch 70/100 – Average Loss: 0.9343\n",
      "Epoch 71/100 – Average Loss: 1.0047\n",
      "Epoch 72/100 – Average Loss: 0.8815\n",
      "Epoch 73/100 – Average Loss: 0.8479\n",
      "Epoch 74/100 – Average Loss: 0.8151\n",
      "Epoch 75/100 – Average Loss: 0.7508\n",
      "Epoch 76/100 – Average Loss: 0.7347\n",
      "Epoch 77/100 – Average Loss: 0.6985\n",
      "Epoch 78/100 – Average Loss: 0.7048\n",
      "Epoch 79/100 – Average Loss: 0.6880\n",
      "Epoch 80/100 – Average Loss: 0.8353\n",
      "Epoch 81/100 – Average Loss: 0.8029\n",
      "Epoch 82/100 – Average Loss: 0.8769\n",
      "Epoch 83/100 – Average Loss: 0.8699\n",
      "Epoch 84/100 – Average Loss: 0.9064\n",
      "Epoch 85/100 – Average Loss: 0.7905\n",
      "Epoch 86/100 – Average Loss: 0.7541\n",
      "Epoch 87/100 – Average Loss: 0.7001\n",
      "Epoch 88/100 – Average Loss: 0.6471\n",
      "Epoch 89/100 – Average Loss: 0.6316\n",
      "Epoch 90/100 – Average Loss: 0.6565\n",
      "Epoch 91/100 – Average Loss: 0.6148\n",
      "Epoch 92/100 – Average Loss: 0.5959\n",
      "Epoch 93/100 – Average Loss: 0.5670\n",
      "Epoch 94/100 – Average Loss: 0.5945\n",
      "Epoch 95/100 – Average Loss: 0.5406\n",
      "Epoch 96/100 – Average Loss: 0.5477\n",
      "Epoch 97/100 – Average Loss: 0.5298\n",
      "Epoch 98/100 – Average Loss: 0.5182\n",
      "Epoch 99/100 – Average Loss: 0.5117\n",
      "Epoch 100/100 – Average Loss: 0.5275\n",
      "----------------------------------------------------------------------------\n",
      "Test Accuracy 62.7%\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Hyper-Parameters and Data Preparation\n",
    "# ----------------------------\n",
    "# Hyper-parameters (matching the TensorFlow script)\n",
    "HIDDEN_UNITS = 100\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "# OUTPUT_DIM is assumed to be the number of classes (len(class_values))\n",
    "# Make sure that you have defined 'class_values' when processing your dataset.\n",
    "OUTPUT_DIM = len(class_values)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 3e-1\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Convert training and test indices/labels to PyTorch tensors if not already done.\n",
    "# (These values are provided by your data preparation code.)\n",
    "node_states = node_states.to(torch_dev)\n",
    "edges = edges.to(torch_dev)\n",
    "train_indices = torch.tensor(train_indices, dtype=torch.long).to(torch_dev)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long).to(torch_dev)\n",
    "test_indices = torch.tensor(test_indices, dtype=torch.long).to(torch_dev)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long).to(torch_dev)\n",
    "\n",
    "# Make sure the node_states and edges are already torch.Tensors.\n",
    "# For example, they may have been defined as:\n",
    "# edges = torch.tensor(citations[[\"target\", \"source\"]].values, dtype=torch.long)\n",
    "# node_states = torch.tensor(papers.sort_values(\"paper_id\").iloc[:, 1:-1].values, dtype=torch.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# Build Model, Loss Function, and Optimizer\n",
    "# ----------------------------\n",
    "model = GraphAttentionNetwork(node_states, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM)\n",
    "model = model.to(torch_dev)\n",
    "\n",
    "# Use SGD with momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "# CrossEntropyLoss in PyTorch combines LogSoftmax and NLLLoss (from logits)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Mini-Batches\n",
    "# ----------------------------\n",
    "# For a graph model, the forward pass computes representations for all nodes.\n",
    "# To mimic batching, we compute the forward pass each mini-batch and then gather the predictions\n",
    "# corresponding to the batch indices.  (For small graphs, the extra forward passes are acceptable.)\n",
    "num_train = len(train_indices)\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set to training mode\n",
    "    permutation = torch.randperm(num_train, device=torch_dev)\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i in range(0, num_train, BATCH_SIZE):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Select mini-batch\n",
    "        batch_perm = permutation[i : i + BATCH_SIZE]\n",
    "        batch_indices = train_indices[batch_perm]\n",
    "        batch_labels = train_labels[batch_perm]\n",
    "\n",
    "        # Forward pass: compute output for entire graph (stored on device)\n",
    "        outputs = model()  # outputs shape: (N, OUTPUT_DIM)\n",
    "        batch_logits = outputs[batch_indices]  # gather logits for mini-batch\n",
    "\n",
    "        loss = loss_fn(batch_logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / (num_train / BATCH_SIZE)\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} – Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation on the Test Set\n",
    "# ----------------------------\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model()\n",
    "    # Gather test node predictions.\n",
    "    test_logits = outputs[test_indices]\n",
    "    # Compute predicted labels.\n",
    "    predicted_labels = torch.argmax(test_logits, dim=1)\n",
    "    correct = (predicted_labels == test_labels).sum().item()\n",
    "    test_accuracy = correct / len(test_labels)\n",
    "\n",
    "print(\"--\" * 38)\n",
    "print(f\"Test Accuracy {test_accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002e529",
   "metadata": {},
   "source": [
    "### Predict (probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85af7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Probabilistic_Methods\n",
      "\tProbability of Case_Based               =   0.296%\n",
      "\tProbability of Genetic_Algorithms       =   0.560%\n",
      "\tProbability of Neural_Networks          =  79.397%\n",
      "\tProbability of Probabilistic_Methods    =  19.085%\n",
      "\tProbability of Reinforcement_Learning   =   0.171%\n",
      "\tProbability of Rule_Learning            =   0.345%\n",
      "\tProbability of Theory                   =   0.144%\n",
      "------------------------------------------------------------\n",
      "Example 2: Reinforcement_Learning\n",
      "\tProbability of Case_Based               =   0.002%\n",
      "\tProbability of Genetic_Algorithms       =   0.000%\n",
      "\tProbability of Neural_Networks          =   0.000%\n",
      "\tProbability of Probabilistic_Methods    =   0.000%\n",
      "\tProbability of Reinforcement_Learning   =  99.855%\n",
      "\tProbability of Rule_Learning            =   0.038%\n",
      "\tProbability of Theory                   =   0.105%\n",
      "------------------------------------------------------------\n",
      "Example 3: Genetic_Algorithms\n",
      "\tProbability of Case_Based               =   3.126%\n",
      "\tProbability of Genetic_Algorithms       =  16.657%\n",
      "\tProbability of Neural_Networks          =   2.851%\n",
      "\tProbability of Probabilistic_Methods    =   0.756%\n",
      "\tProbability of Reinforcement_Learning   =  44.581%\n",
      "\tProbability of Rule_Learning            =  19.975%\n",
      "\tProbability of Theory                   =  12.053%\n",
      "------------------------------------------------------------\n",
      "Example 4: Probabilistic_Methods\n",
      "\tProbability of Case_Based               =   0.008%\n",
      "\tProbability of Genetic_Algorithms       =   0.006%\n",
      "\tProbability of Neural_Networks          =  25.015%\n",
      "\tProbability of Probabilistic_Methods    =  74.950%\n",
      "\tProbability of Reinforcement_Learning   =   0.007%\n",
      "\tProbability of Rule_Learning            =   0.009%\n",
      "\tProbability of Theory                   =   0.005%\n",
      "------------------------------------------------------------\n",
      "Example 5: Genetic_Algorithms\n",
      "\tProbability of Case_Based               =   3.957%\n",
      "\tProbability of Genetic_Algorithms       =  54.200%\n",
      "\tProbability of Neural_Networks          =  15.377%\n",
      "\tProbability of Probabilistic_Methods    =   1.307%\n",
      "\tProbability of Reinforcement_Learning   =   3.775%\n",
      "\tProbability of Rule_Learning            =  15.534%\n",
      "\tProbability of Theory                   =   5.849%\n",
      "------------------------------------------------------------\n",
      "Example 6: Genetic_Algorithms\n",
      "\tProbability of Case_Based               =   3.957%\n",
      "\tProbability of Genetic_Algorithms       =  54.200%\n",
      "\tProbability of Neural_Networks          =  15.377%\n",
      "\tProbability of Probabilistic_Methods    =   1.307%\n",
      "\tProbability of Reinforcement_Learning   =   3.775%\n",
      "\tProbability of Rule_Learning            =  15.534%\n",
      "\tProbability of Theory                   =   5.849%\n",
      "------------------------------------------------------------\n",
      "Example 7: Neural_Networks\n",
      "\tProbability of Case_Based               =   0.000%\n",
      "\tProbability of Genetic_Algorithms       =   0.000%\n",
      "\tProbability of Neural_Networks          = 100.000%\n",
      "\tProbability of Probabilistic_Methods    =   0.000%\n",
      "\tProbability of Reinforcement_Learning   =   0.000%\n",
      "\tProbability of Rule_Learning            =   0.000%\n",
      "\tProbability of Theory                   =   0.000%\n",
      "------------------------------------------------------------\n",
      "Example 8: Case_Based\n",
      "\tProbability of Case_Based               =   2.773%\n",
      "\tProbability of Genetic_Algorithms       =  13.555%\n",
      "\tProbability of Neural_Networks          =   2.216%\n",
      "\tProbability of Probabilistic_Methods    =   0.652%\n",
      "\tProbability of Reinforcement_Learning   =  50.612%\n",
      "\tProbability of Rule_Learning            =  18.524%\n",
      "\tProbability of Theory                   =  11.669%\n",
      "------------------------------------------------------------\n",
      "Example 9: Reinforcement_Learning\n",
      "\tProbability of Case_Based               =   0.000%\n",
      "\tProbability of Genetic_Algorithms       =   0.000%\n",
      "\tProbability of Neural_Networks          =   0.000%\n",
      "\tProbability of Probabilistic_Methods    =   0.000%\n",
      "\tProbability of Reinforcement_Learning   =  99.979%\n",
      "\tProbability of Rule_Learning            =   0.005%\n",
      "\tProbability of Theory                   =   0.016%\n",
      "------------------------------------------------------------\n",
      "Example 10: Probabilistic_Methods\n",
      "\tProbability of Case_Based               =   0.000%\n",
      "\tProbability of Genetic_Algorithms       =   0.000%\n",
      "\tProbability of Neural_Networks          =   1.645%\n",
      "\tProbability of Probabilistic_Methods    =  98.355%\n",
      "\tProbability of Reinforcement_Learning   =   0.000%\n",
      "\tProbability of Rule_Learning            =   0.000%\n",
      "\tProbability of Theory                   =   0.000%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode and compute predictions.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the full graph; outputs has shape (N, OUTPUT_DIM)\n",
    "    outputs = model()\n",
    "    # Gather logits for test node indices.\n",
    "    test_logits = outputs[test_indices]\n",
    "    # Convert logits to probabilities using softmax along the class dimension.\n",
    "    test_probs = F.softmax(test_logits, dim=1)\n",
    "\n",
    "# Create a mapping from label integer to the class name.\n",
    "mapping = {v: k for k, v in class_idx.items()}\n",
    "\n",
    "# Print probabilities for the first 10 test examples.\n",
    "for i, (probs, label) in enumerate(zip(test_probs[:10], test_labels[:10])):\n",
    "    # If label is a tensor, convert to int.\n",
    "    label_int = int(label) if torch.is_tensor(label) else label\n",
    "    print(f\"Example {i+1}: {mapping[label_int]}\")\n",
    "    # probs is a tensor; converting to list for easy iteration.\n",
    "    for j, c in zip(probs.tolist(), class_idx.keys()):\n",
    "        print(f\"\\tProbability of {c: <24} = {j * 100:7.3f}%\")\n",
    "    print(\"---\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-course-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
